model:
  f1: 8
  d: 2
  f2: 16
  dropout: 0.5
  nb_classes: 4
  chans: 22
  samples: 1125

training:
  batch_size: 64
  lr: 0.001
  epochs: 100
  patience: 20
